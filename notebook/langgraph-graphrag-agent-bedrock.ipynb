{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from boto3 import client\n",
    "from botocore.config import Config\n",
    "\n",
    "# Increase Bedrock read timeout\n",
    "config = Config(read_timeout=1000)\n",
    "client = client(service_name=\"bedrock-runtime\", config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.chat_models.bedrock import ChatBedrock\n",
    "\n",
    "# Use claude 3.5 for creating cypher query and generating graph database\n",
    "advanced_llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n",
    "    temperature=0,\n",
    "    max_tokens=1000,\n",
    "    model_kwargs={\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    },\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Use less complex llm model for simple request routing task, e.g. Mistral 8*7B\n",
    "basic_llm = ChatBedrock(\n",
    "    model_id=\"mistral.mixtral-8x7b-instruct-v0:1\",\n",
    "    temperature=0,\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws.embeddings import BedrockEmbeddings\n",
    "\n",
    "embeddings = BedrockEmbeddings(\n",
    "    model_id=\"amazon.titan-embed-text-v2:0\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deine a Web Search Agent to Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "\n",
    "search_query = \"agent OR 'large language model' OR 'prompt engineering'\"\n",
    "max_results = 2\n",
    "\n",
    "# Fetch papers from arXiv\n",
    "client = arxiv.Client()\n",
    "search = arxiv.Search(\n",
    "    query=search_query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "\n",
    "docs = []\n",
    "for result in client.results(search):\n",
    "    docs.append(\n",
    "        {\"title\": result.title, \"summary\": result.summary, \"url\": result.entry_id}\n",
    "    )\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=500, chunk_overlap=50\n",
    ")\n",
    "doc_splits = text_splitter.create_documents(\n",
    "    [doc[\"summary\"] for doc in docs], metadatas=docs\n",
    ")\n",
    "\n",
    "print(f\"Number of papers: {len(docs)}\")\n",
    "print(f\"Number of chunks: {len(doc_splits)}\")\n",
    "\n",
    "\n",
    "# Add to Milvus\n",
    "vectorstore = Milvus.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag_milvus\",\n",
    "    embedding=embeddings,\n",
    "    connection_args={\"uri\": \"./milvus_ingest.db\"},\n",
    "    index_params={\"index_type\": \"FLAT\"},\n",
    ")\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Database Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_neo4j.graphs.neo4j_graph import Neo4jGraph\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "\n",
    "graph = Neo4jGraph()\n",
    "\n",
    "graph_transformer = LLMGraphTransformer(\n",
    "    llm=advanced_llm,\n",
    "    allowed_nodes=[\"Paper\", \"Author\", \"Topic\"],\n",
    "    node_properties=[\"title\", \"summary\", \"url\"],\n",
    "    allowed_relationships=[\"AUTHORED\", \"DISCUSSES\", \"RELATED_TO\"],\n",
    ")\n",
    "\n",
    "graph_documents = graph_transformer.convert_to_graph_documents(doc_splits)\n",
    "\n",
    "graph.add_graph_documents(graph_documents)\n",
    "\n",
    "print(f\"Graph documents: {len(graph_documents)}\")\n",
    "print(f\"Nodes from 1st graph doc:{graph_documents[0].nodes}\")\n",
    "print(f\"Relationships from 1st graph doc:{graph_documents[0].relationships}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Native RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Context: {context} \n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"document\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | basic_llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define GraphRAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GraphRAG setup\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_neo4j.chains.graph_qa.cypher import GraphCypherQAChain\n",
    "\n",
    "\n",
    "cypher_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an expert at generating Cypher queries for Neo4j.\n",
    "    Use the following schema to generate a Cypher query that answers the given question.\n",
    "    Make the query flexible by using case-insensitive matching and partial string matching where appropriate.\n",
    "    Focus on searching paper titles as they contain the most relevant information.\n",
    "    \n",
    "    Schema:\n",
    "    {schema}\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Cypher Query:\"\"\",\n",
    "    input_variables=[\"schema\", \"question\"],\n",
    ")\n",
    "\n",
    "\n",
    "# QA prompt\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following Cypher query results to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise. If topic information is not available, focus on the paper titles.\n",
    "    \n",
    "    Question: {question} \n",
    "    Cypher Query: {query}\n",
    "    Query Results: {context} \n",
    "    \n",
    "    Answer:\"\"\",\n",
    "    input_variables=[\"question\", \"query\", \"context\"],\n",
    ")\n",
    "\n",
    "\n",
    "# Chain\n",
    "graph_rag_chain = GraphCypherQAChain.from_llm(\n",
    "    allow_dangerous_requests=True,\n",
    "    cypher_llm=advanced_llm,\n",
    "    qa_llm=advanced_llm,\n",
    "    validate_cypher=True,\n",
    "    graph=graph,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    "    return_direct=True,\n",
    "    cypher_prompt=cypher_prompt,\n",
    "    qa_prompt=qa_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example input data\n",
    "# question = \"What techniques are used for Multi-Agent? \"\n",
    "question = \"What paper talk about Multi-Agent?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_context = rag_chain.invoke({\"context\": format_docs(retriever.invoke(question)), \"question\": question})\n",
    "\n",
    "print(vector_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_context = graph_rag_chain.invoke({\"query\": question})\n",
    "\n",
    "print(graph_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Composite Vector + Graph Generations\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Prompt\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context from a vector store and a graph database to answer the question. If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise:\n",
    "    Question: {question} \n",
    "    Vector Context: {context} \n",
    "    Graph Context: {graph_context}\n",
    "    Answer: \n",
    "    \"\"\",\n",
    "    input_variables=[\"question\", \"context\", \"graph_context\"],\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the chain\n",
    "composite_chain = prompt | basic_llm | StrOutputParser()\n",
    "answer = composite_chain.invoke(\n",
    "    {\"question\": question, \"context\": vector_context, \"graph_context\": graph_context}\n",
    ")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
